{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8b5447d-dee7-402a-8ca1-a045f47ec866",
   "metadata": {},
   "source": [
    "# 00 - Environment Setup and Configuration\n",
    "This notebook is responsible for:\n",
    "- Recording environment and dependency versions\n",
    "- Setting random seeds to ensure reproducibility\n",
    "- Detecting GPU information\n",
    "- Creating the project directory structure\n",
    "- Saving configuration information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16287951-5a0c-48a4-b31b-9c9ab9529715",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a82547e-c1c6-4ab8-a954-f3cbc6bba9ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "import evaluate\n",
    "import peft\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541af974-20d4-4dee-9a35-ca97e27eb144",
   "metadata": {},
   "source": [
    "## 2. Record Environment Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b50f3d4d-838b-4541-b9bf-7ac695a6f60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python_version: 3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]\n",
      "pytorch_version: 2.6.0+cu124\n",
      "transformers_version: 4.57.1\n",
      "datasets_version: 4.2.0\n",
      "evaluate_version: 0.4.6\n",
      "peft_version: 0.17.1\n",
      "accelerate_version: 1.10.1\n",
      "cuda_available: True\n",
      "cuda_version: 12.4\n",
      "device_count: 1\n",
      "timestamp: 2025-10-17 22:13:29\n"
     ]
    }
   ],
   "source": [
    "# Collect Environment Information\n",
    "env_info = {\n",
    "    \"python_version\": sys.version,\n",
    "    \"pytorch_version\": torch.__version__,\n",
    "    \"transformers_version\": transformers.__version__,\n",
    "    \"datasets_version\": datasets.__version__,\n",
    "    \"evaluate_version\": evaluate.__version__,\n",
    "    \"peft_version\": peft.__version__,\n",
    "    \"accelerate_version\": accelerate.__version__,\n",
    "    \"cuda_available\": torch.cuda.is_available(),\n",
    "    \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"N/A\",\n",
    "    \"device_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "# Print environment information\n",
    "for key, value in env_info.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccbb26b-bc12-4977-ba76-417ca7c29240",
   "metadata": {},
   "source": [
    "## 3. GPU Information Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be2d5ae1-799a-4503-b457-13eaffcfd067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Information:\n",
      "Number of GPUs: 1\n",
      "\n",
      "GPU 0:\n",
      "  Name: Tesla V100-SXM2-32GB\n",
      "  Total Memory: 31.73 GB\n",
      "  Compute Capability: 7.0\n",
      "\n",
      "Currently Allocated Memory: 0.00 GB\n",
      "Currently Reserved Memory: 0.00 GB\n",
      "\n",
      "Default Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Detailed GPU Information\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Information:\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\nGPU {i}:\")\n",
    "        print(f\"  Name: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Total Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"  Compute Capability: {torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}\")\n",
    "    \n",
    "    # Current memory usage\n",
    "    print(f\"\\nCurrently Allocated Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"Currently Reserved Memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU detected. The CPU will be used for training (which may be slower).\")\n",
    "\n",
    "# Set default device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nDefault Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c7497-9ac2-4ece-9a59-b72fabf503b9",
   "metadata": {},
   "source": [
    "## 4. Set Random Seeds (Ensure Reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c24d519f-ea5d-4005-ac44-60dbb63de737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed has been set to: 42\n",
      "This ensures experiment reproducibility.\n"
     ]
    }
   ],
   "source": [
    "# Fix random seed\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def set_seed(seed=RANDOM_SEED):\n",
    "    \"\"\"Set all random seeds to ensure reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # Ensure deterministic behavior in PyTorch\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set environment variable\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "print(f\"Random seed has been set to: {RANDOM_SEED}\")\n",
    "print(\"This ensures experiment reproducibility.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340194b-9448-4c13-b6f7-2e92ecc5f843",
   "metadata": {},
   "source": [
    "## 5. Create Project Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fafd45d9-9837-4fd1-aa91-646444bcf1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: data\n",
      "Created directory: data/processed\n",
      "Created directory: outputs\n",
      "Created directory: outputs/roberta_full\n",
      "Created directory: outputs/roberta_lora\n",
      "Created directory: outputs/distilbert_full\n",
      "Created directory: outputs/distilbert_lora\n",
      "Created directory: logs\n",
      "Created directory: reports\n",
      "Created directory: configs\n",
      "\n",
      "Project directory structure has been successfully created.\n"
     ]
    }
   ],
   "source": [
    "# Define project directory structure\n",
    "PROJECT_ROOT = Path(\".\")\n",
    "DIRS = {\n",
    "    \"data\": PROJECT_ROOT / \"data\",\n",
    "    \"data_processed\": PROJECT_ROOT / \"data\" / \"processed\",\n",
    "    \"outputs\": PROJECT_ROOT / \"outputs\",\n",
    "    \"outputs_roberta_full\": PROJECT_ROOT / \"outputs\" / \"roberta_full\",\n",
    "    \"outputs_roberta_lora\": PROJECT_ROOT / \"outputs\" / \"roberta_lora\",\n",
    "    \"outputs_distilbert_full\": PROJECT_ROOT / \"outputs\" / \"distilbert_full\",\n",
    "    \"outputs_distilbert_lora\": PROJECT_ROOT / \"outputs\" / \"distilbert_lora\",\n",
    "    \"logs\": PROJECT_ROOT / \"logs\",\n",
    "    \"reports\": PROJECT_ROOT / \"reports\",\n",
    "    \"configs\": PROJECT_ROOT / \"configs\"\n",
    "}\n",
    "\n",
    "# Create all directories\n",
    "for name, path in DIRS.items():\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Created directory: {path}\")\n",
    "\n",
    "print(\"\\nProject directory structure has been successfully created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bcdc8f-4f89-45e8-9ca3-6b59751c58dc",
   "metadata": {},
   "source": [
    "## 6. Define Project Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c899d63d-3fa7-4c77-a680-6e375a10d05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Configuration Parameters:\n",
      "{\n",
      "  \"project_name\": \"Legal QA Fine-tuning\",\n",
      "  \"random_seed\": 42,\n",
      "  \"device\": \"cuda\",\n",
      "  \"dataset_name\": \"isaacus/LegalQAEval\",\n",
      "  \"train_split_ratio\": 0.9,\n",
      "  \"valid_split_ratio\": 0.1,\n",
      "  \"models\": {\n",
      "    \"roberta\": \"roberta-base\",\n",
      "    \"distilbert\": \"distilbert-base-uncased\"\n",
      "  },\n",
      "  \"max_length\": 384,\n",
      "  \"doc_stride\": 128,\n",
      "  \"max_answer_length\": 30,\n",
      "  \"training\": {\n",
      "    \"learning_rate\": 3e-05,\n",
      "    \"batch_size\": 8,\n",
      "    \"num_epochs\": 3,\n",
      "    \"warmup_steps\": 500,\n",
      "    \"weight_decay\": 0.01,\n",
      "    \"evaluation_strategy\": \"epoch\",\n",
      "    \"save_strategy\": \"epoch\",\n",
      "    \"logging_steps\": 100,\n",
      "    \"fp16\": true\n",
      "  },\n",
      "  \"lora\": {\n",
      "    \"r\": 8,\n",
      "    \"lora_alpha\": 16,\n",
      "    \"target_modules\": [\n",
      "      \"query\",\n",
      "      \"value\"\n",
      "    ],\n",
      "    \"lora_dropout\": 0.05,\n",
      "    \"bias\": \"none\",\n",
      "    \"task_type\": \"QUESTION_ANS\"\n",
      "  },\n",
      "  \"paths\": {\n",
      "    \"data\": \"data\",\n",
      "    \"data_processed\": \"data/processed\",\n",
      "    \"outputs\": \"outputs\",\n",
      "    \"outputs_roberta_full\": \"outputs/roberta_full\",\n",
      "    \"outputs_roberta_lora\": \"outputs/roberta_lora\",\n",
      "    \"outputs_distilbert_full\": \"outputs/distilbert_full\",\n",
      "    \"outputs_distilbert_lora\": \"outputs/distilbert_lora\",\n",
      "    \"logs\": \"logs\",\n",
      "    \"reports\": \"reports\",\n",
      "    \"configs\": \"configs\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Project configuration\n",
    "PROJECT_CONFIG = {\n",
    "    # Basic information\n",
    "    \"project_name\": \"Legal QA Fine-tuning\",\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"device\": str(device),\n",
    "    \n",
    "    # Data configuration\n",
    "    \"dataset_name\": \"isaacus/LegalQAEval\",\n",
    "    \"train_split_ratio\": 0.9,\n",
    "    \"valid_split_ratio\": 0.1,\n",
    "    \n",
    "    # Model configuration\n",
    "    \"models\": {\n",
    "        \"roberta\": \"roberta-base\",\n",
    "        \"distilbert\": \"distilbert-base-uncased\"\n",
    "    },\n",
    "    \n",
    "    # Preprocessing configuration\n",
    "    \"max_length\": 384,  # Common maximum length for QA tasks\n",
    "    \"doc_stride\": 128,\n",
    "    \"max_answer_length\": 30,\n",
    "    \n",
    "    # Training configuration\n",
    "    \"training\": {\n",
    "        \"learning_rate\": 3e-5,\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 3,\n",
    "        \"warmup_steps\": 500,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"logging_steps\": 100,\n",
    "        \"fp16\": torch.cuda.is_available()  # Use mixed precision only when GPU is available\n",
    "    },\n",
    "    \n",
    "    # LoRA configuration\n",
    "    \"lora\": {\n",
    "        \"r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"target_modules\": [\"query\", \"value\"],\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"bias\": \"none\",\n",
    "        \"task_type\": \"QUESTION_ANS\"\n",
    "    },\n",
    "    \n",
    "    # Directory paths\n",
    "    \"paths\": {k: str(v) for k, v in DIRS.items()}\n",
    "}\n",
    "\n",
    "print(\"Project Configuration Parameters:\")\n",
    "print(json.dumps(PROJECT_CONFIG, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c23c2-b9f7-48b7-889c-194979fdc725",
   "metadata": {},
   "source": [
    "## 7. Save Configuration to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e00bf891-5e49-4434-85ed-66de94198982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration has been saved to: configs/project_config.json\n",
      "Environment information has been saved to: configs/environment_info.json\n"
     ]
    }
   ],
   "source": [
    "# Save project configuration file\n",
    "config_path = DIRS[\"configs\"] / \"project_config.json\"\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(PROJECT_CONFIG, f, indent=2, ensure_ascii=False)\n",
    "print(f\"Configuration has been saved to: {config_path}\")\n",
    "\n",
    "# Save environment information\n",
    "env_path = DIRS[\"configs\"] / \"environment_info.json\"\n",
    "with open(env_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(env_info, f, indent=2, ensure_ascii=False)\n",
    "print(f\"Environment information has been saved to: {env_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2135fe-41bf-44e5-8ed6-244a540ac42a",
   "metadata": {},
   "source": [
    "## 8. Create Utility Functions File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23349c7d-2b21-425d-bd36-73adb9825e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions file has been created: utils.py\n"
     ]
    }
   ],
   "source": [
    "# Create a general utility functions file for use in later notebooks\n",
    "utils_code = '''\n",
    "\"\"\"General Utility Functions\"\"\"\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def load_config(config_path=\"configs/project_config.json\"):\n",
    "    \"\"\"Load project configuration\"\"\"\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_model_size(model):\n",
    "    \"\"\"Calculate model parameter size\"\"\"\n",
    "    param_size = 0\n",
    "    trainable_params = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.nelement()\n",
    "    \n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    total_size = (param_size + buffer_size) / 1024**2\n",
    "    total_params = sum(p.nelement() for p in model.parameters())\n",
    "    \n",
    "    return {\n",
    "        \"total_params\": total_params,\n",
    "        \"trainable_params\": trainable_params,\n",
    "        \"size_mb\": total_size\n",
    "    }\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Print GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "    else:\n",
    "        print(\"GPU not in use\")\n",
    "'''\n",
    "\n",
    "utils_path = PROJECT_ROOT / \"utils.py\"\n",
    "with open(utils_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(utils_code)\n",
    "print(f\"Utility functions file has been created: {utils_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (ML)",
   "language": "python",
   "name": "ml310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
