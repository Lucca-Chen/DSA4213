{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fee18b3-697a-4832-89fc-c47b958b4b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: Tesla V100-SXM2-32GB\n",
      "\n",
      "Data sizes:\n",
      "  Train: 1629\n",
      "  Valid: 182\n",
      "  Test:  453\n",
      "Number of labels: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/pbs.2192.cbis-pbs/ipykernel_2542169/2941140925.py:117: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LoRA parameters:\n",
      "  Trainable: 3,247,107\n",
      "  Total: 127,895,046\n",
      "  Trainable %: 2.54%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='306' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [306/306 00:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.392500</td>\n",
       "      <td>0.215805</td>\n",
       "      <td>0.950549</td>\n",
       "      <td>0.932892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.045407</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.984567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>0.077557</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.984567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training time: 22.9s\n",
      "Peak VRAM: 2266 MiB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VALIDATION RESULTS\n",
      "============================================================\n",
      "Accuracy:  0.9835\n",
      "Macro-F1:  0.9846\n",
      "Loss:      0.0454\n",
      "\n",
      "============================================================\n",
      "TEST RESULTS\n",
      "============================================================\n",
      "Accuracy:  0.9691\n",
      "Macro-F1:  0.9582\n",
      "\n",
      "Classification Report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9219    0.9672    0.9440        61\n",
      "           1     0.9856    0.9820    0.9838       278\n",
      "           2     0.9554    0.9386    0.9469       114\n",
      "\n",
      "    accuracy                         0.9691       453\n",
      "   macro avg     0.9543    0.9626    0.9582       453\n",
      "weighted avg     0.9694    0.9691    0.9691       453\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x420 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ERROR ANALYSIS\n",
      "============================================================\n",
      "Total errors: 14\n",
      "Error rate: 3.09%\n",
      "\n",
      "First 10 errors (index, true_label, pred_label):\n",
      "  Index 105: True=2, Pred=1\n",
      "  Index 127: True=1, Pred=0\n",
      "  Index 139: True=2, Pred=0\n",
      "  Index 157: True=2, Pred=0\n",
      "  Index 172: True=1, Pred=2\n",
      "  Index 177: True=2, Pred=1\n",
      "  Index 183: True=1, Pred=0\n",
      "  Index 212: True=2, Pred=0\n",
      "  Index 373: True=1, Pred=2\n",
      "  Index 404: True=0, Pred=2\n",
      "\n",
      "============================================================\n",
      "SUMMARY - LoRA r={LORA_R}\n",
      "============================================================\n",
      "Method: LoRA Fine-tuning\n",
      "Model: roberta-base\n",
      "LoRA r: 16, alpha: 32, dropout: 0.1\n",
      "Trainable params: 3,247,107 (2.54%)\n",
      "Training time: 22.9s\n",
      "Peak VRAM: 2266 MiB\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ABLATION STUDY: Testing different LoRA r values\n",
      "============================================================\n",
      "\n",
      "Training with r=4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/pbs.2192.cbis-pbs/ipykernel_2542169/2941140925.py:230: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='306' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [306/306 00:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.366300</td>\n",
       "      <td>0.149702</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.954587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.164000</td>\n",
       "      <td>0.073070</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.979001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>0.071571</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.989643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with r=8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/pbs.2192.cbis-pbs/ipykernel_2542169/2941140925.py:230: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='306' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [306/306 00:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.386900</td>\n",
       "      <td>0.154099</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.952505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.128500</td>\n",
       "      <td>0.078320</td>\n",
       "      <td>0.972527</td>\n",
       "      <td>0.968373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.068309</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.984180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with r=16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/pbs.2192.cbis-pbs/ipykernel_2542169/2941140925.py:230: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='306' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [306/306 00:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.496200</td>\n",
       "      <td>0.339192</td>\n",
       "      <td>0.818681</td>\n",
       "      <td>0.554467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.149200</td>\n",
       "      <td>0.057657</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.984359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.059031</td>\n",
       "      <td>0.994505</td>\n",
       "      <td>0.994856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with r=32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/tmp/pbs.2192.cbis-pbs/ipykernel_2542169/2941140925.py:230: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='306' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [306/306 00:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.377100</td>\n",
       "      <td>0.199586</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.911328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.141200</td>\n",
       "      <td>0.071469</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.979179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.084614</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.979179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ABLATION RESULTS\n",
      "============================================================\n",
      " r  trainable_params  trainable_pct  dev_accuracy   dev_f1  test_accuracy  test_f1  wall_time_sec  peak_vram_mib\n",
      " 4           1256451       0.997941      0.989011 0.989643        0.97351 0.963018      22.927825           2236\n",
      " 8           1920003       1.516974      0.989011 0.984180        0.97351 0.965250      23.171749           2246\n",
      "16           3247107       2.538884      0.994505 0.994856        0.97351 0.964547      23.149690           2267\n",
      "32           5901315       4.520374      0.983516 0.979179        0.97351 0.963304      23.838228           2305\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import time, random, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, TrainingArguments, Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Configuration\n",
    "AGREE_SPLIT = \"sentences_allagree\"\n",
    "PARQUET_PATH = f\"hf://datasets/takala/financial_phrasebank@refs/convert/parquet/{AGREE_SPLIT}/train/*.parquet\"\n",
    "\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "SEED = 42\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16\n",
    "LR_LORA = 2e-4  # Higher LR for LoRA\n",
    "MAX_LEN = 256\n",
    "WARMUP_RATIO = 0.06\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# LoRA settings\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Load and split dataset\n",
    "raw = load_dataset(\"parquet\", data_files={\"train\": PARQUET_PATH})\n",
    "dataset_split = raw[\"train\"].train_test_split(test_size=0.2, seed=SEED, stratify_by_column=\"label\")\n",
    "tmp = dataset_split[\"train\"].train_test_split(test_size=0.1, seed=SEED, stratify_by_column=\"label\")\n",
    "train_ds, valid_ds, test_ds = tmp[\"train\"], tmp[\"test\"], dataset_split[\"test\"]\n",
    "\n",
    "print(\"\\nData sizes:\")\n",
    "print(f\"  Train: {len(train_ds)}\")\n",
    "print(f\"  Valid: {len(valid_ds)}\")\n",
    "print(f\"  Test:  {len(test_ds)}\")\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"sentence\"], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "train_tok = train_ds.map(tokenize_function, batched=True).rename_column(\"label\", \"labels\")\n",
    "valid_tok = valid_ds.map(tokenize_function, batched=True).rename_column(\"label\", \"labels\")\n",
    "test_tok = test_ds.map(tokenize_function, batched=True).rename_column(\"label\", \"labels\")\n",
    "\n",
    "keep = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "for ds in (train_tok, valid_tok, test_tok):\n",
    "    ds.remove_columns([c for c in ds.column_names if c not in keep])\n",
    "    ds.set_format(type=\"torch\", columns=keep)\n",
    "\n",
    "num_labels = int(len(set(train_ds[\"label\"])))\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "\n",
    "# Build RoBERTa with LoRA\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "\n",
    "target_modules = [\"query\", \"key\", \"value\", \"dense\"]\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "model = get_peft_model(base_model, lora_cfg)\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nLoRA parameters:\")\n",
    "print(f\"  Trainable: {trainable:,}\")\n",
    "print(f\"  Total: {total:,}\")\n",
    "print(f\"  Trainable %: {100.0 * trainable / total:.2f}%\")\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_runs\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR_LORA,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=20,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    report_to=[\"none\"],\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=valid_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Track training time and memory\n",
    "start = time.time()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "train_output = trainer.train()\n",
    "wall_time_sec = time.time() - start\n",
    "peak_mem = int(torch.cuda.max_memory_allocated() / 1024**2) if torch.cuda.is_available() else None\n",
    "print(f\"\\nTraining time: {wall_time_sec:.1f}s\")\n",
    "if peak_mem:\n",
    "    print(f\"Peak VRAM: {peak_mem} MiB\")\n",
    "\n",
    "# Evaluation\n",
    "dev_metrics = trainer.evaluate(valid_tok)\n",
    "pred = trainer.predict(test_tok)\n",
    "y_true = np.array(test_ds[\"label\"])\n",
    "y_pred = np.argmax(pred.predictions, axis=-1)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {dev_metrics.get('eval_accuracy', 0.0):.4f}\")\n",
    "print(f\"Macro-F1:  {dev_metrics.get('eval_f1_macro', 0.0):.4f}\")\n",
    "print(f\"Loss:      {dev_metrics.get('eval_loss', 0.0):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Macro-F1:  {f1m:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (TEST):\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "labels_order = sorted(list(set(y_true.tolist())))\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels_order)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4.8, 4.2))\n",
    "im = ax.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set(\n",
    "    xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]),\n",
    "    xticklabels=labels_order, yticklabels=labels_order,\n",
    "    ylabel=\"True label\", xlabel=\"Predicted label\",\n",
    ")\n",
    "plt.setp(ax.get_xticklabels(), rotation=30, ha=\"right\", rotation_mode=\"anchor\")\n",
    "thr = cm.max() / 2\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], \"d\"), ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thr else \"black\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Error analysis\n",
    "errors = [(i, int(gt), int(pred)) for i, (gt, pred) in enumerate(zip(y_true, y_pred)) if int(gt) != int(pred)]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total errors: {len(errors)}\")\n",
    "print(f\"Error rate: {100.0 * len(errors) / len(y_true):.2f}%\")\n",
    "print(f\"\\nFirst 10 errors (index, true_label, pred_label):\")\n",
    "for idx, gt, pred in errors[:10]:\n",
    "    print(f\"  Index {idx}: True={gt}, Pred={pred}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY - LoRA r={LORA_R}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Method: LoRA Fine-tuning\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"LoRA r: {LORA_R}, alpha: {LORA_ALPHA}, dropout: {LORA_DROPOUT}\")\n",
    "print(f\"Trainable params: {trainable:,} ({100.0 * trainable / total:.2f}%)\")\n",
    "print(f\"Training time: {wall_time_sec:.1f}s\")\n",
    "if peak_mem:\n",
    "    print(f\"Peak VRAM: {peak_mem} MiB\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ablation study: test different r values\n",
    "DO_ABLATION = True\n",
    "\n",
    "if DO_ABLATION:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ABLATION STUDY: Testing different LoRA r values\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    rows = []\n",
    "    for r in [4, 8, 16, 32]:\n",
    "        print(f\"\\nTraining with r={r}...\")\n",
    "        \n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "        lora_cfg = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS, r=r, lora_alpha=LORA_ALPHA,\n",
    "            lora_dropout=LORA_DROPOUT, target_modules=target_modules\n",
    "        )\n",
    "        model = get_peft_model(base_model, lora_cfg)\n",
    "\n",
    "        trn = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        tot = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_tok,\n",
    "            eval_dataset=valid_tok,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "        start = time.time()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        trainer.train()\n",
    "        wall = time.time() - start\n",
    "        peak = int(torch.cuda.max_memory_allocated() / 1024**2) if torch.cuda.is_available() else None\n",
    "\n",
    "        dev_m = trainer.evaluate(valid_tok)\n",
    "        pred = trainer.predict(test_tok)\n",
    "        y_pred = np.argmax(pred.predictions, axis=-1)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "        rows.append({\n",
    "            \"r\": r,\n",
    "            \"trainable_params\": trn,\n",
    "            \"trainable_pct\": 100.0 * trn / tot,\n",
    "            \"dev_accuracy\": dev_m.get(\"eval_accuracy\", 0.0),\n",
    "            \"dev_f1\": dev_m.get(\"eval_f1_macro\", 0.0),\n",
    "            \"test_accuracy\": acc,\n",
    "            \"test_f1\": f1m,\n",
    "            \"wall_time_sec\": wall,\n",
    "            \"peak_vram_mib\": peak,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values(\"r\").reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ABLATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (ML)",
   "language": "python",
   "name": "ml310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
